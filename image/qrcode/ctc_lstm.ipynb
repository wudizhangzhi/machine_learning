{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import codecs\n",
    "import re\n",
    "import datetime\n",
    "# import cairocffi as cairo\n",
    "# import editdistance\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "# import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks\n",
    "import pathlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 60, 200, 1)\n",
      "[[[[0.79215686]\n",
      "   [0.79215686]\n",
      "   [0.79215686]]\n",
      "\n",
      "  [[0.79215686]\n",
      "   [0.79215686]\n",
      "   [0.79215686]]\n",
      "\n",
      "  [[0.79215686]\n",
      "   [0.79215686]\n",
      "   [0.79215686]]]\n",
      "\n",
      "\n",
      " [[[0.80784314]\n",
      "   [0.80784314]\n",
      "   [0.80784314]]\n",
      "\n",
      "  [[0.80784314]\n",
      "   [0.80784314]\n",
      "   [0.80784314]]\n",
      "\n",
      "  [[0.80784314]\n",
      "   [0.80784314]\n",
      "   [0.80784314]]]\n",
      "\n",
      "\n",
      " [[[0.77254902]\n",
      "   [0.77254902]\n",
      "   [0.77254902]]\n",
      "\n",
      "  [[0.77254902]\n",
      "   [0.77254902]\n",
      "   [0.77254902]]\n",
      "\n",
      "  [[0.77254902]\n",
      "   [0.77254902]\n",
      "   [0.77254902]]]]\n",
      "[[11. 12. 16.  6. 13.  8.]\n",
      " [ 9. 25.  2.  5. 34.  6.]\n",
      " [25.  3. 30. 30. 20.  9.]]\n",
      "[[48.]\n",
      " [48.]\n",
      " [48.]]\n",
      "[[6.]\n",
      " [6.]\n",
      " [6.]]\n",
      "['bcg6d8', '9p25y6', 'p3uuk9']\n"
     ]
    }
   ],
   "source": [
    "from icp_factory import GenCaptcha\n",
    "\n",
    "gencaptcha = GenCaptcha()\n",
    "\n",
    "class TextImageGenerator(keras.callbacks.Callback):\n",
    "    # 所有可能字符\n",
    "    LABELS = '0123456789abcdefghijklmnopqrstuvwxyz '\n",
    "    \n",
    "    def __init__(self, train_path, validate_path, img_w, img_h, channel, downsample_factor, absolute_max_string_len=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_path: 训练数据路径\n",
    "            validate_path: 验证图片路径\n",
    "            img_w:\n",
    "            img_h:\n",
    "            downsample_factor: TODO 未知\n",
    "            absolute_max_string_len: 最大字符串长度\n",
    "        \"\"\"\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.channel = channel\n",
    "        self.train_path = train_path\n",
    "        self.validate_path = validate_path\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.blank_label = self.get_output_size() - 1\n",
    "        self.absolute_max_string_len = absolute_max_string_len\n",
    "        # 数据\n",
    "        self.train_imgs = self.get_all_imgs(self.train_path)\n",
    "        self.validate_imgs = self.get_all_imgs(self.validate_path)\n",
    "        self.cur_idx = 0\n",
    "        \n",
    "        np.random.shuffle(self.train_imgs)\n",
    "        np.random.shuffle(self.validate_imgs)\n",
    "    \n",
    "    def get_all_imgs(self, path):\n",
    "#         p = pathlib.Path(path)\n",
    "        # jpg or png\n",
    "#         return list([str(i) for i in p.glob('*.jpg')])\n",
    "        return [os.path.join(path, i) for i in os.listdir(path)]\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        return len(self.LABELS) + 1\n",
    "    \n",
    "    def char2idx(self, char):\n",
    "        idx = self.LABELS.find(char.lower())\n",
    "        return idx if idx != -1 else self.blank_label\n",
    "    \n",
    "    @staticmethod\n",
    "    def labels_to_text(labels):\n",
    "        ret = []\n",
    "        for c in labels:\n",
    "            if c == len(TextImageGenerator.LABELS):  # CTC Blank\n",
    "                ret.append(\"\")\n",
    "            else:\n",
    "                ret.append(TextImageGenerator.LABELS[c])\n",
    "        return \"\".join(ret)\n",
    "    \n",
    "    def path2matrix(self, path):\n",
    "        \"\"\"\n",
    "        input shape: (batch_size, w, h, channel)\n",
    "        \"\"\"\n",
    "        img = cv2.imread(path)\n",
    "        img = self.formatCaptcha(img)\n",
    "        return img\n",
    "    \n",
    "    @classmethod\n",
    "    def formatCaptcha(cls, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img/ 255.\n",
    "#         img_transpose = np.einsum('hw->wh', img)\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        return img\n",
    "    \n",
    "    def _get_one_captcha(self):\n",
    "        captcha, text = gencaptcha.gen_one()\n",
    "        return captcha, text\n",
    "\n",
    "    def get_next_batch(self, paths, batch_size=32):\n",
    "        def get_label(img_path):\n",
    "            \"\"\"\n",
    "            获取验证码对应的字符串\n",
    "            \"\"\"\n",
    "            return os.path.basename(img_path).split('.')[0].lower()\n",
    "        i = 0\n",
    "#         X_data = np.zeros((batch_size, self.img_w, self.img_h, self.channel))\n",
    "        X_data = np.zeros((batch_size, self.img_h, self.img_w, self.channel))\n",
    "        labels = np.zeros((batch_size, self.absolute_max_string_len))\n",
    "        input_length = np.zeros([batch_size, 1])\n",
    "        label_length = np.zeros([batch_size, 1])\n",
    "        source_str = []\n",
    "        while i < batch_size:\n",
    "            if self.cur_idx >= len(paths):\n",
    "                # 归零，洗牌\n",
    "                self.cur_idx = 0\n",
    "                np.random.shuffle(paths)\n",
    "#             img_path = paths[self.cur_idx]\n",
    "#             label_text = get_label(img_path)\n",
    "            # 使用自动生成的\n",
    "            captcha, label_text = self._get_one_captcha()\n",
    "#             X_data[i, :] = self.path2matrix(img_path)\n",
    "            X_data[i, :] = self.formatCaptcha(captcha)\n",
    "            input_length[i] = self.img_w // self.downsample_factor - 2\n",
    "            label_length[i] = len(label_text)\n",
    "            labels[i] = [self.char2idx(char) for char in label_text]\n",
    "            source_str.append(label_text)\n",
    "            \n",
    "            self.cur_idx += 1\n",
    "            i += 1\n",
    "            \n",
    "        inputs = {\n",
    "              'the_input': X_data,\n",
    "              'the_labels': labels,\n",
    "              'input_length': input_length,\n",
    "              'label_length': label_length,\n",
    "              'source_str': source_str  # used for visualization only\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([batch_size])}\n",
    "        return (inputs, outputs)\n",
    "            \n",
    "    def get_next_train(self, batch_size=32):\n",
    "        while True:\n",
    "            yield self.get_next_batch(self.train_imgs, batch_size)\n",
    "    \n",
    "    def get_next_val(self, batch_size=100):\n",
    "        while True:\n",
    "            yield self.get_next_batch(self.validate_imgs, batch_size)\n",
    "        \n",
    "\n",
    "\n",
    "# train_path = 'E:\\\\traindata\\\\captcha_create\\\\train'\n",
    "# validate_path = 'E:\\\\traindata\\\\captcha_create\\\\test'\n",
    "train_path = '/media/moon/0000678400004823/data/train'\n",
    "validate_path = '/media/moon/0000678400004823/data/test'\n",
    "# test_img = os.path.join(train_path, '00ARLO.jpg')\n",
    "\n",
    "img_w = 200\n",
    "img_h = 60\n",
    "channel = 1\n",
    "downsample_factor = 4\n",
    "img_gen = TextImageGenerator(train_path, validate_path, img_w, img_h, channel, downsample_factor)\n",
    "ret_input, ret_output = next(img_gen.get_next_train(3))\n",
    "ret_input, ret_output = next(img_gen.get_next_train(3))\n",
    "print(ret_input['the_input'].shape)\n",
    "print(ret_input['the_input'][:3, :3, :3])\n",
    "print(ret_input['the_labels'])\n",
    "print(ret_input['input_length'])\n",
    "print(ret_input['label_length'])\n",
    "print(ret_input['source_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 24, 69, 1)\n",
      "[[[[0.32941176]\n",
      "   [0.32941176]\n",
      "   [0.32941176]]\n",
      "\n",
      "  [[0.32941176]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.32941176]\n",
      "   [1.        ]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.32941176]\n",
      "   [0.32941176]\n",
      "   [0.32941176]]\n",
      "\n",
      "  [[0.32941176]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.32941176]\n",
      "   [1.        ]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.32941176]\n",
      "   [0.32941176]\n",
      "   [0.32941176]]\n",
      "\n",
      "  [[0.32941176]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.32941176]\n",
      "   [1.        ]\n",
      "   [1.        ]]]]\n",
      "[[33. 28.  0.  0.  0.  0.]\n",
      " [26. 29. 20. 20.  0.  0.]\n",
      " [ 1.  4.  0.  0.  0.  0.]]\n",
      "[[15.]\n",
      " [15.]\n",
      " [15.]]\n",
      "[[2.]\n",
      " [4.]\n",
      " [2.]]\n",
      "['xs', 'qtkk', '14']\n"
     ]
    }
   ],
   "source": [
    "class CustomTextImageGenerator(TextImageGenerator):\n",
    "    def get_next_batch(self, paths, batch_size=32):\n",
    "        def get_label(img_path):\n",
    "            \"\"\"\n",
    "            获取验证码对应的字符串\n",
    "            1538839826_hdgz.png\n",
    "            \"\"\"\n",
    "            return os.path.basename(img_path).split('_')[-1].split('.')[0].lower()\n",
    "        i = 0\n",
    "#         X_data = np.zeros((batch_size, self.img_w, self.img_h, self.channel))\n",
    "        X_data = np.zeros((batch_size, self.img_h, self.img_w, self.channel))\n",
    "        labels = np.zeros((batch_size, self.absolute_max_string_len))\n",
    "        input_length = np.zeros([batch_size, 1])\n",
    "        label_length = np.zeros([batch_size, 1])\n",
    "        source_str = []\n",
    "        while i < batch_size:\n",
    "            if self.cur_idx >= len(paths):\n",
    "                # 归零，洗牌\n",
    "                self.cur_idx = 0\n",
    "                np.random.shuffle(paths)\n",
    "            img_path = paths[self.cur_idx]\n",
    "            label_text = get_label(img_path)\n",
    "            # 使用自动生成的\n",
    "            img = self.path2matrix(img_path)\n",
    "            X_data[i, :, :img.shape[1], :] = img\n",
    "#             X_data[i, :] = self.formatCaptcha(captcha)\n",
    "            input_length[i] = self.img_w // self.downsample_factor - 2\n",
    "            label_length[i] = len(label_text)\n",
    "            labels[i][:len(label_text)] = [self.char2idx(char) for char in label_text]\n",
    "            source_str.append(label_text)\n",
    "            \n",
    "            self.cur_idx += 1\n",
    "            i += 1\n",
    "            \n",
    "        inputs = {\n",
    "              'the_input': X_data,\n",
    "              'the_labels': labels,\n",
    "              'input_length': input_length,\n",
    "              'label_length': label_length,\n",
    "              'source_str': source_str  # used for visualization only\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([batch_size])}\n",
    "        return (inputs, outputs)\n",
    "    \n",
    "train_path = '/media/moon/0000678400004823/qrcode'\n",
    "validate_path = '/media/moon/0000678400004823/qrcode_test'\n",
    "# test_img = os.path.join(train_path, '00ARLO.jpg')\n",
    "\n",
    "img_w = 69\n",
    "img_h = 24\n",
    "channel = 1\n",
    "downsample_factor = 4\n",
    "img_gen = CustomTextImageGenerator(train_path, validate_path, img_w, img_h, channel, downsample_factor)\n",
    "ret_input, ret_output = next(img_gen.get_next_train(3))\n",
    "ret_input, ret_output = next(img_gen.get_next_train(3))\n",
    "print(ret_input['the_input'].shape)\n",
    "print(ret_input['the_input'][:3, :3, :3])\n",
    "print(ret_input['the_labels'])\n",
    "print(ret_input['input_length'])\n",
    "print(ret_input['label_length'])\n",
    "print(ret_input['source_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 24, 69, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 24, 69, 16)   160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 12, 34, 16)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 12, 34, 16)   2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 6, 17, 16)    0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 17, 96)       0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 17, 32)       3104        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 17, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 17, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 17, 512)      0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 17, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 17, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 17, 1024)     0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 17, 38)       38950       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 17, 38)       0           dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,867,574\n",
      "Trainable params: 4,867,574\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moon/.virtualenvs/bdzh/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"th..., outputs=Tensor(\"so...)`\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 16\n",
    "kernel_size = (3, 3)\n",
    "pool_size = 2\n",
    "time_dense_size = 32\n",
    "rnn_size = 512\n",
    "minibatch_size = 32\n",
    "OUTPUT_DIR = 'E:\\\\Workplace\\\\bdzh\\\\MachineLearning\\\\SmallCaptcha\\\\image_ocr'\n",
    "\n",
    "\n",
    "# the actual loss calc occurs here despite it not being\n",
    "# an internal Keras loss function\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (channel, img_w, img_h)\n",
    "else:\n",
    "#     input_shape = (img_w, img_h, channel)\n",
    "    input_shape = (img_h, img_w, channel)\n",
    "\n",
    "act = 'relu'\n",
    "input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
    "inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "               activation=act, kernel_initializer='he_normal',\n",
    "               name='conv1')(input_data)\n",
    "inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
    "inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "               activation=act, kernel_initializer='he_normal',\n",
    "               name='conv2')(inner)\n",
    "inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
    "\n",
    "conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n",
    "inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
    "\n",
    "# cuts down input size going into RNN:\n",
    "inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
    "\n",
    "# Two layers of bidirectional GRUs\n",
    "# GRU seems to work as well, if not better than LSTM:\n",
    "gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\n",
    "gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\n",
    "gru1_merged = add([gru_1, gru_1b])\n",
    "gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
    "gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
    "\n",
    "# transforms RNN output to character activations:\n",
    "inner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal',\n",
    "              name='dense2')(concatenate([gru_2, gru_2b]))\n",
    "y_pred = Activation('softmax', name='softmax')(inner)\n",
    "base_model = Model(input=input_data, output=y_pred)\n",
    "base_model.summary()\n",
    "\n",
    "labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "# clipnorm seems to speeds up convergence\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VizCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n",
    "        self.test_func = test_func\n",
    "        self.output_dir = os.path.join(\n",
    "            OUTPUT_DIR, run_name)\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.num_display_words = num_display_words\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def show_edit_distance(self, num):\n",
    "        num_left = num\n",
    "        mean_norm_ed = 0.0\n",
    "        mean_ed = 0.0\n",
    "        while num_left > 0:\n",
    "            word_batch = next(self.text_img_gen)[0]\n",
    "            num_proc = min(word_batch['the_input'].shape[0], num_left)\n",
    "            decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n",
    "            for j in range(num_proc):\n",
    "                edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n",
    "                mean_ed += float(edit_dist)\n",
    "                mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n",
    "            num_left -= num_proc\n",
    "        mean_norm_ed = mean_norm_ed / num\n",
    "        mean_ed = mean_ed / num\n",
    "        print('\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f'\n",
    "              % (num, mean_ed, mean_norm_ed))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0: # 每10个周期计算一次正确率\n",
    "            word_batch = next(self.text_img_gen)[0]\n",
    "            res = decode_batch(self.test_func, word_batch['the_input'])\n",
    "            if word_batch['the_input'][0].shape[0] < 256:\n",
    "                cols = 2\n",
    "            else:\n",
    "                cols = 1\n",
    "            acc = 0\n",
    "            total = word_batch['the_input'].shape[0]\n",
    "            for i in range(total):\n",
    "                if word_batch['source_str'][i].lower() == res[i].lower():\n",
    "                    acc += 1\n",
    "            acc_ratio = 100 * acc / total\n",
    "            print('正确率: %0.5f' % acc_ratio)\n",
    "            if acc_ratio > 50:\n",
    "                self.model.save_weights(os.path.join(self.output_dir, 'weights%02d_acc_%0.5f.h5' % (epoch, acc_ratio)))\n",
    "        word_batch = next(self.text_img_gen)[0]\n",
    "        res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n",
    "        if word_batch['the_input'][0].shape[0] < 256:\n",
    "            cols = 2\n",
    "        else:\n",
    "            cols = 1\n",
    "        for i in range(self.num_display_words):\n",
    "            plt.subplot(self.num_display_words // cols, cols, i + 1)\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                the_input = word_batch['the_input'][i, 0, :, :]\n",
    "            else:\n",
    "                the_input = word_batch['the_input'][i, :, :, 0]\n",
    "            plt.imshow(the_input, cmap='Greys_r')\n",
    "            plt.xlabel('T = \\'%s\\' Decoed = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n",
    "        plt.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n",
    "        plt.close()\n",
    "\n",
    "def decode_batch(test_func, word_batch):\n",
    "    out = test_func([word_batch])[0]\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = TextImageGenerator.labels_to_text(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def evaluate(test_func, img_gen, batch_size=5):\n",
    "    correct_count = 0\n",
    "    img_gen = img_gen.get_next_val(batch_size=batch_size)\n",
    "    _x, _ctc = next(img_gen)\n",
    "    for i in range(batch_size):\n",
    "        test_X = _x['the_input'][i]\n",
    "        test_c = _x['source_str'][i]\n",
    "        test_X = np.expand_dims(test_X, axis=-1)\n",
    "        result = decode_batch(test_func, test_X)[0]\n",
    "        try:\n",
    "            if test_c.lower() == result.lower():\n",
    "                correct_count += 1\n",
    "                # print(\"[INFO] actual: %s, predict: %s\" % (test_c, result))\n",
    "            else:\n",
    "                print(\"[ERROR] actual: %s, predict: %s\" % (test_c, result))\n",
    "        except Exception as e:\n",
    "            print(e.message)\n",
    "    print(\"Accuracy: %.2f%%\" % ((float(correct_count) / batch_size) * 100))\n",
    "    \n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self, test_func, text_img_gen):\n",
    "        self.test_func = test_func\n",
    "        self.text_img_gen = text_img_gen\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        evaluate(self.test_func, self.text_img_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "# captures output of softmax so we can decode the output during visualization\n",
    "test_func = K.function([input_data], [y_pred])\n",
    "\n",
    "viz_cb = VizCallback(datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'), test_func, img_gen.get_next_val())\n",
    "# evaluate_gen = TextImageGenerator(train_path, validate_path, img_w, img_h, channel, downsample_factor)\n",
    "# evaluator = Evaluator(test_func, evaluate_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 12.4013 - val_loss: 10.0194\n",
      "正确率: 0.00000\n",
      "Epoch 2/400\n",
      "100/100 [==============================] - 57s 567ms/step - loss: 8.6069 - val_loss: 6.7020\n",
      "Epoch 3/400\n",
      "100/100 [==============================] - 57s 570ms/step - loss: 5.2409 - val_loss: 3.9868\n",
      "Epoch 4/400\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 2.9628 - val_loss: 2.3589\n",
      "Epoch 5/400\n",
      "100/100 [==============================] - 57s 572ms/step - loss: 1.6813 - val_loss: 1.5971\n",
      "Epoch 6/400\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 0.9493 - val_loss: 1.2268\n",
      "Epoch 7/400\n",
      "100/100 [==============================] - 61s 610ms/step - loss: 0.4991 - val_loss: 1.0018\n",
      "Epoch 8/400\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 0.2676 - val_loss: 0.8756\n",
      "Epoch 9/400\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 0.1488 - val_loss: 0.8138\n",
      "Epoch 10/400\n",
      "100/100 [==============================] - 63s 627ms/step - loss: 0.0881 - val_loss: 0.7363\n",
      "Epoch 11/400\n",
      "100/100 [==============================] - 63s 627ms/step - loss: 0.0554 - val_loss: 0.7268\n",
      "正确率: 84.00000\n",
      "Epoch 12/400\n",
      "100/100 [==============================] - 63s 628ms/step - loss: 0.0383 - val_loss: 0.6811\n",
      "Epoch 13/400\n",
      "100/100 [==============================] - 63s 626ms/step - loss: 0.0424 - val_loss: 0.6763\n",
      "Epoch 14/400\n",
      "100/100 [==============================] - 63s 635ms/step - loss: 0.0236 - val_loss: 0.6735\n",
      "Epoch 15/400\n",
      "100/100 [==============================] - 64s 637ms/step - loss: 0.0189 - val_loss: 0.6644\n",
      "Epoch 16/400\n",
      "100/100 [==============================] - 64s 641ms/step - loss: 0.0161 - val_loss: 0.6650\n",
      "Epoch 17/400\n",
      "100/100 [==============================] - 63s 634ms/step - loss: 0.0142 - val_loss: 0.6588\n",
      "Epoch 18/400\n",
      "100/100 [==============================] - 63s 634ms/step - loss: 0.0127 - val_loss: 0.6557\n",
      "Epoch 19/400\n",
      "100/100 [==============================] - 65s 655ms/step - loss: 0.0115 - val_loss: 0.6449\n",
      "Epoch 20/400\n",
      "100/100 [==============================] - 65s 646ms/step - loss: 0.0105 - val_loss: 0.6763\n",
      "Epoch 21/400\n",
      "100/100 [==============================] - 64s 635ms/step - loss: 0.0097 - val_loss: 0.6472\n",
      "正确率: 85.00000\n",
      "Epoch 22/400\n",
      "100/100 [==============================] - 64s 641ms/step - loss: 0.0090 - val_loss: 0.6562\n",
      "Epoch 23/400\n",
      "  5/100 [>.............................] - ETA: 47s - loss: 0.0082"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f07b679b1648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mviz_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdzh/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "model.fit_generator(generator=img_gen.get_next_train(100),\n",
    "                    steps_per_epoch=100,\n",
    "                    epochs=400,\n",
    "                    validation_data=img_gen.get_next_val(1000),\n",
    "                    validation_steps=5,\n",
    "                    callbacks=[viz_cb],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "generator = img_gen.get_next_val(2)\n",
    "_x_input, _  = next(generator)\n",
    "_X_test = _x_input['the_input']\n",
    "_y_test = _x_input['the_labels']\n",
    "\n",
    "img = _X_test[0]\n",
    "print(img.shape)\n",
    "print(np.einsum('hwc->whc', img).shape)\n",
    "print(img[:, :, 0].T.shape)\n",
    "print(np.max(img.flatten()))\n",
    "print(np.min(img.flatten()))\n",
    "print(_X_test[0].flatten().shape)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img[:, :, 0].T)\n",
    "plt.subplot(1, 2, 2)\n",
    "img = np.einsum('hwc->whc', img)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# for i in range(_y_pred.shape[0]):\n",
    "#     _y_pred = base_model.predict(np.expand_dims(_X_test[i], axis=0))[:, 2:, :]\n",
    "#     print(_y_pred.shape)\n",
    "#     for i in range(_y_pred.shape[0]):\n",
    "#         __X_test = _X_test[i]\n",
    "#         __y_test = _y_test[i]\n",
    "#         __y_pred = _y_pred[i]\n",
    "#         __y_pred = np.expand_dims(__y_pred, axis=0)\n",
    "#         shape = __y_pred.shape\n",
    "#         ctc_decode = K.ctc_decode(__y_pred, \n",
    "#                                   input_length=np.ones(shape[0])*shape[1])[0][0]\n",
    "#         out = K.get_value(ctc_decode)[0]\n",
    "#         print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "from IPython.display import Image\n",
    "Image(filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_path = 'E:\\\\traindata\\\\captcha_create\\\\train'\n",
    "validate_path = 'E:\\\\traindata\\\\captcha_create\\\\test'\n",
    "test_img = os.path.join(train_path, '00ARLO.jpg')\n",
    "\n",
    "img_w = 200\n",
    "img_h = 60\n",
    "channel = 3\n",
    "downsample_factor = 4\n",
    "gen = TextImageGenerator(train_path, validate_path, img_w, img_h, channel, downsample_factor)\n",
    "for i in range(40):\n",
    "    next(gen.get_next_train(500))\n",
    "\n",
    "imgs = next(gen.get_next_train(2))\n",
    "_x, _ctc = imgs[0], imgs[1]\n",
    "\n",
    "_the_input = _x['the_input']\n",
    "_the_labels = _x['the_labels']\n",
    "_input_length = _x['input_length']\n",
    "_label_length = _x['label_length']\n",
    "print(_ctc)\n",
    "num = _the_input.shape[0]\n",
    "\n",
    "for i in range(num):\n",
    "    img = _the_input[i]\n",
    "    print(img.shape)\n",
    "    plt.subplot(1, num, i + 1)\n",
    "    plt.xlabel('label: %s' % _the_labels[i])\n",
    "    plt.imshow(img[:, :, 0], cmap='Greys_r')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('E:\\\\Workplace\\\\bdzh\\\\MachineLearning\\\\SmallCaptcha\\\\image_ocr\\\\2019_05_23_23_32_26\\\\weights400.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "from user_agent import generate_user_agent\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# from models import predict_images\n",
    "import numpy as np\n",
    "from string import digits, ascii_lowercase\n",
    "# import dataset\n",
    "import cv2\n",
    "\n",
    "CHAPTER_LIST = digits + ascii_lowercase\n",
    "\n",
    "CHAPTER_LIST = CHAPTER_LIST.replace('0', '').replace('1', '').replace('2', '')\n",
    "\n",
    "\n",
    "class IcpMemoInfoClient(object):\n",
    "    TIMEOUT = 10\n",
    "    MAX_ERROR_COUNT = 10\n",
    "    # CHAPTER_LIST = digits + ascii_lowercase\n",
    "    HOST = 'beian.miit.gov.cn'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sess = requests.Session()\n",
    "        # 初始化session\n",
    "        self._init_sess()\n",
    "\n",
    "    def _init_sess(self):\n",
    "        headers = {\n",
    "            'User-Agent': generate_user_agent()\n",
    "        }\n",
    "        self.sess.headers = headers\n",
    "\n",
    "    def _request(self, method, uri, **kwargs):\n",
    "        if method.lower() == 'post':\n",
    "            request_method = self.sess.post\n",
    "        else:\n",
    "            request_method = self.sess.get\n",
    "        return request_method('http://%s/%s' % (self.HOST, uri), timeout=self.TIMEOUT, **kwargs)\n",
    "\n",
    "    def _split_image(self, img, num=6):\n",
    "        \"\"\"\n",
    "        分割图片\n",
    "        Args:\n",
    "            img: \n",
    "            num: 6\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        height, width = img.shape\n",
    "        d = math.ceil(width / num)\n",
    "        croped_images = []\n",
    "        for i in range(num):\n",
    "            start, end = i * d, (i + 1) * d\n",
    "            if end > width:\n",
    "                start, end = width - d, width\n",
    "            img_croped = img[:, start: end]\n",
    "            croped_images.append(np.copy(img_croped))\n",
    "        return croped_images\n",
    "\n",
    "    def _predict_images(self, images):\n",
    "        \"\"\"\n",
    "        预测单一的字符的图片的列表\n",
    "        Args:\n",
    "            img: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # 利用模型预测\n",
    "        # 转化为np array\n",
    "        # images_array = np.array([self._image2matrix(img) for img in images])\n",
    "        idxs = predict_images(images)\n",
    "        # idx2char\n",
    "        ret = ''.join([self._idx2char(i) for i in idxs])\n",
    "        return ret\n",
    "\n",
    "    def _idx2char(self, idx):\n",
    "        return CHAPTER_LIST[idx]\n",
    "\n",
    "    def _image2matrix(self, image):\n",
    "        # array = np.array(image.getdata())\n",
    "        image.resize((image.shape[0], image.shape[1], 1))\n",
    "        return image\n",
    "\n",
    "    def _prepare_captcha(self, image):\n",
    "        \"\"\"\n",
    "        预处理图片\n",
    "        \"\"\"\n",
    "        # 转化为黑白\n",
    "        # image = image.convert('LA')\n",
    "        # TODO 降噪\n",
    "        # gray_src = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # gray_src = cv2.bitwise_not(gray_src)\n",
    "        #\n",
    "        # # 二值化\n",
    "        # binary_src = cv2.adaptiveThreshold(gray_src, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, -2)\n",
    "        # # 提取水平线    src.shape[1]得到src列数\n",
    "        # # hline = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1), (-1, -1))\n",
    "        #\n",
    "        # hline = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 4), (-1, -1))  # 定义结构元素，卷积核\n",
    "        # # print('hline: %s' % hline)\n",
    "        # # 提取垂直线    src.shape[0]得到src行数\n",
    "        # vline = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 1), (-1, -1))\n",
    "        # # vline = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        # # 这两步就是形态学的开操作——先腐蚀再膨胀\n",
    "        # # temp = cv2.erode(binary_src, hline)     #腐蚀\n",
    "        # # dst = cv2.dilate(temp, hline)           #膨胀\n",
    "        # # 开运算\n",
    "        # dst = cv2.morphologyEx(binary_src, cv2.MORPH_OPEN, hline)  # 水平方向\n",
    "        # dst = cv2.morphologyEx(dst, cv2.MORPH_OPEN, vline)  # 垂直方向\n",
    "        # # 将二指图片的效果反转既黑色变白色，白色变黑色。 非操作\n",
    "        # dst = cv2.bitwise_not(dst)\n",
    "        image = dataset._denoise(image)\n",
    "        imgs_splited = dataset._split_image(image, size=(32, 32))\n",
    "        # 转化为rgb channel 3\n",
    "        outputs = [cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in imgs_splited]\n",
    "        return outputs\n",
    "\n",
    "    def predict_captcha(self, images):\n",
    "        \"\"\"\n",
    "        预测验证码\n",
    "        Args:\n",
    "            image: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        # images = self._split_image(image)\n",
    "        return self._predict_images(images)\n",
    "\n",
    "    def get_captcha(self):\n",
    "        \"\"\"\n",
    "        获取验证码\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        url = 'getVerifyCode?%d' % randint(0, 100)\n",
    "        return self._request('get', url).content\n",
    "\n",
    "    def verify_code(self, code):\n",
    "        url = 'common/validate/validCode.action'\n",
    "        data = {\n",
    "            'validateValue': code\n",
    "        }\n",
    "        ret = None\n",
    "        try:\n",
    "            ret = self._request('post', url, data=data)\n",
    "            result = ret.json()['result']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if ret:\n",
    "                print(ret.text)\n",
    "            result = False\n",
    "        return result\n",
    "\n",
    "    def process_captcha(self):\n",
    "        # 获取验证码\n",
    "        captcha_bytes = self.get_captcha()\n",
    "        # 处理验证码\n",
    "        captcha_io = BytesIO(captcha_bytes)\n",
    "        # captcha = Image.open(captcha_io)\n",
    "        captcha_io.seek(0)\n",
    "        captcha_bytes = np.asarray(bytearray(captcha_io.read()), dtype=np.uint8)\n",
    "        captcha = cv2.imdecode(captcha_bytes, cv2.IMREAD_COLOR)\n",
    "        captcha = self._prepare_captcha(captcha)\n",
    "        # 分析验证码\n",
    "        captcha_text = self.predict_captcha(captcha)\n",
    "        # 验证验证码\n",
    "        ret = self.verify_code(captcha_text)\n",
    "        return ret, captcha_text, captcha\n",
    "\n",
    "    def search(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: \n",
    "                data = {\n",
    "                    'siteName': '',\n",
    "                    'condition': 1,\n",
    "                    'siteDomain': '',\n",
    "                    'siteUrl': '',\n",
    "                    'mainLicense': '',\n",
    "                    'siteIp': '',\n",
    "                    'unitName': '',\n",
    "                    'mainUnitNature': -1,\n",
    "                    'certType': -1,\n",
    "                    'mainUnitCertNo': '',\n",
    "                    'verifyCode': captcha_text\n",
    "                }\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        url = 'http://www.miitbeian.gov.cn/icp/publish/query/icpMemoInfo_searchExecute.action'\n",
    "        err_count = 0\n",
    "        while err_count < self.MAX_ERROR_COUNT:\n",
    "            is_captcha_validate, captcha_text = self.process_captcha()\n",
    "            if is_captcha_validate:\n",
    "                break\n",
    "        if not is_captcha_validate:\n",
    "            raise Exception('验证码验证失败: %s' % captcha_text)\n",
    "        data = {\n",
    "            'siteName': '',\n",
    "            'condition': 1,\n",
    "            'siteDomain': '',\n",
    "            'siteUrl': '',\n",
    "            'mainLicense': '',\n",
    "            'siteIp': '',\n",
    "            'unitName': '',\n",
    "            'mainUnitNature': -1,\n",
    "            'certType': -1,\n",
    "            'mainUnitCertNo': '',\n",
    "            'verifyCode': captcha_text\n",
    "        }\n",
    "        ret = self._request('post', url, data=data)\n",
    "        # TODO 处理html\n",
    "        return ret\n",
    "    \n",
    "client = IcpMemoInfoClient()\n",
    "captcha_bytes = client.get_captcha()\n",
    "# 处理验证码\n",
    "captcha_io = BytesIO(captcha_bytes)\n",
    "# captcha = Image.open(captcha_io)\n",
    "captcha_io.seek(0)\n",
    "captcha_bytes = np.asarray(bytearray(captcha_io.read()), dtype=np.uint8)\n",
    "captcha_icp = cv2.imdecode(captcha_bytes, cv2.IMREAD_COLOR)\n",
    "captcha, text = gencaptcha.gen_one()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(captcha, cmap='Greys_r')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(captcha_icp, cmap='Greys_r')\n",
    "\n",
    "model.load_weights('./weights/weights400.h5')\n",
    "# 格式化图片\n",
    "captcha = TextImageGenerator.formatCaptcha(captcha)\n",
    "captcha = np.expand_dims(captcha, axis=0)\n",
    "ret = decode_batch(test_func, captcha)\n",
    "print('预测自动生成的: %s' % ret)\n",
    "\n",
    "\n",
    "captcha_icp = TextImageGenerator.formatCaptcha(captcha_icp)\n",
    "captcha_icp = np.expand_dims(captcha_icp, axis=0)\n",
    "ret = decode_batch(test_func, captcha_icp)\n",
    "print('预测icp: %s' % ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 52, 1)\n",
      "预测: ['pd7']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC9CAYAAACj84abAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAC15JREFUeJzt3V+IHed9xvHvI1nGxQrYihwhbLdOW0HQRaPAIlziC8UhRk1D5UIxcdKiC4N6kYADSYubGyeFgAP501yEghoL6yJxapq4FsW0FqrByY3jdeLUsp1i19hEQtbGcUOsm8Ryfrk4Y7IW2j2r82923/P9wOHMvGd25zevdh4Nc96ZSVUhSdr4NvVdgCRpMgx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMuG+eHk+wHvgZsBr5RVfestvymy6+sTVdsG2eVkjR33nz91KtVdc2w5UYO9CSbga8DHwJOAU8kOVZVz670M5uu2MZVN9456iolaS79/PjfvbyW5cY55bIXeKGqXqyqXwPfBg6M8fskSWMYJ9CvBX66bP5U1/Y2SQ4lWUyyWG+cG2N1kqTVTP1L0ao6XFULVbWQLVunvTpJmlvjBPpp4Ppl89d1bZKkHowzyuUJYFeSdzMI8o8CHxvlF736yGfGKEOS2rD9li+N9fMjB3pVnU/ySeC/GAxbPFJVz4xVjSRpZGONQ6+qh4GHJ1SLJGkMXikqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRjriUVan5L0uv6q6nX9q5lV36znPrhU9tnG4RG6JDXCQJekRhjoktQIA12SGmGgS1IjxhrlkuQl4HXgTeB8VS1MoqhW9T36ZFZW286VRjK01jetbY82hkkMW/xAVb06gd8jSRqDp1wkqRHjBnoBjyR5MsmhSRQkSRrNuKdcbqqq00neBRxP8pOqemz5Al3QHwLYdMVVY65OkrSSsY7Qq+p0974EPAjsvcgyh6tqoaoWsmXrOKuTJK1i5EBPcmWSd7w1DdwCnJxUYZKkSzPOKZcdwIPd8KzLgG9V1X9OpCqtSd83MxplaF5LN3raiEMTR+mXSW9n33+3LRs50KvqReC9E6xFkjQGhy1KUiMMdElqhIEuSY0w0CWpET6CThrRaqM1JjkyZFajQjZizXo7j9AlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AhvzqVVzerxYxvxcW6zslrfXOpNsHycXNs8QpekRhjoktQIA12SGmGgS1IjDHRJasTQQE9yJMlSkpPL2rYlOZ7k+e796umWKUkaZi1H6PcB+y9ouws4UVW7gBPdvDaAJJf0krRxDA30qnoMeO2C5gPA0W76KHDrhOuSJF2iUc+h76iqM930K8COCdUjSRrR2F+K1uBSsRUvF0tyKMliksV649y4q5MkrWDUQD+bZCdA97600oJVdbiqFqpqIVu2jrg6SdIwowb6MeBgN30QeGgy5UiSRjX05lxJ7gf2AduTnALuBu4BHkhyB/AycNs0i9TF9T0KZZQbbfVdc2tW6k9vmjWfhgZ6Vd2+wkcfnHAtkqQxeKWoJDXCQJekRhjoktQIA12SGuEj6Dawvh/nNskRFn3XvJ7NS386Mmd8HqFLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRjhscQr6HjK2nod/zUvfTHI7+x5quZ7/nvR2HqFLUiMMdElqhIEuSY0w0CWpEQa6JDXCUS5TMKubZm3E0Qct9c16rrnvkTHqh0foktQIA12SGmGgS1IjDHRJaoSBLkmNGBroSY4kWUpyclnb55KcTvJU9/rwdMuUJA2zliP0+4D9F2n/alXt6V4PT7YsabaSXPJrFFW14ksa19BAr6rHgNdmUIskaQzjnEP/ZJL/6U7JXD2xiiRJIxk10P8Z+CNgD3AG+PJKCyY5lGQxyWK9cW7E1UmShhkp0KvqbFW9WVW/Af4F2LvKsoeraqGqFrJl66h1SpKGGCnQk+xcNvuXwMmVlpUkzcbQm3MluR/YB2xPcgq4G9iXZA9QwEvA306xRumS9X1zKketqA9DA72qbr9I871TqEWSNAavFJWkRhjoktQIA12SGmGgS1IjDHRJaoTPFFWTHDZ4cfZL2zxCl6RGGOiS1AgDXZIaYaBLUiMMdElqhKNcZsgRBiuzbybL/pxPHqFLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRFDAz3J9UkeTfJskmeS3Nm1b0tyPMnz3fvV0y9XkrSStRyhnwc+XVW7gRuBTyTZDdwFnKiqXcCJbl6S1JOhgV5VZ6rqh93068BzwLXAAeBot9hR4NZpFSlJGu6SzqEnuQF4H/A4sKOqznQfvQLsWOFnDiVZTLJYb5wbo1RJ0mrWHOhJtgLfAT5VVb9c/lkN7qZ/0TvqV9XhqlqoqoVs2TpWsZKkla0p0JNsYRDm36yq73bNZ5Ps7D7fCSxNp0RJ0lqsZZRLgHuB56rqK8s+OgYc7KYPAg9NvjxJ0lqt5Zmi7wf+Bng6yVNd22eBe4AHktwBvAzcNp0SJUlrMTTQq+r7QFb4+IOTLUeSNCqvFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhqxlptzTd32W77UdwmStOF5hC5JjTDQJakRBrokNcJAl6RGGOiS1IhU1exWlvyMwePqALYDr85s5evTvPeB2z/f2w/2wVq3/w+q6pphC8000N+24mSxqhZ6Wfk6Me994PbP9/aDfTDp7feUiyQ1wkCXpEb0GeiHe1z3ejHvfeD2a977YKLb39s5dEnSZHnKRZIa0UugJ9mf5H+TvJDkrj5qmKUkR5IsJTm5rG1bkuNJnu/er+6zxmlKcn2SR5M8m+SZJHd27fPUB1ck+UGSH3d98Pmu/d1JHu/2hX9NcnnftU5Tks1JfpTkP7r5edv+l5I8neSpJItd28T2g5kHepLNwNeBPwN2A7cn2T3rOmbsPmD/BW13ASeqahdwoptv1Xng01W1G7gR+ET3bz5PffAr4Oaqei+wB9if5Ebgi8BXq+qPgf8H7uixxlm4E3hu2fy8bT/AB6pqz7LhihPbD/o4Qt8LvFBVL1bVr4FvAwd6qGNmquox4LULmg8AR7vpo8CtMy1qhqrqTFX9sJt+ncEOfS3z1QdVVee62S3dq4CbgX/r2pvugyTXAX8OfKObD3O0/auY2H7QR6BfC/x02fyprm3e7KiqM930K8COPouZlSQ3AO8DHmfO+qA73fAUsAQcB/4P+EVVne8WaX1f+Cfg74HfdPPvZL62Hwb/iT+S5Mkkh7q2ie0H6+IBF/OuqipJ88ONkmwFvgN8qqp+OThAG5iHPqiqN4E9Sa4CHgTe03NJM5PkI8BSVT2ZZF/f9fTopqo6neRdwPEkP1n+4bj7QR9H6KeB65fNX9e1zZuzSXYCdO9LPdczVUm2MAjzb1bVd7vmueqDt1TVL4BHgT8Frkry1oFVy/vC+4G/SPISg9OsNwNfY362H4CqOt29LzH4T30vE9wP+gj0J4Bd3bfblwMfBY71UEffjgEHu+mDwEM91jJV3bnSe4Hnquoryz6apz64pjsyJ8nvAR9i8F3Co8BfdYs12wdV9Q9VdV1V3cBgn//vqvo4c7L9AEmuTPKOt6aBW4CTTHA/6OXCoiQfZnA+bTNwpKq+MPMiZijJ/cA+BndWOwvcDfw78ADw+wzuQHlbVV34xWkTktwEfA94mt+dP/0sg/Po89IHf8LgC6/NDA6kHqiqf0zyhwyOWLcBPwL+uqp+1V+l09edcvlMVX1knra/29YHu9nLgG9V1ReSvJMJ7QdeKSpJjfBKUUlqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ijfgs1FkMB1sS3TAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4c2380198>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "path = '/home/moon/test.png'\n",
    "captcha = cv2.imread(path)\n",
    "\n",
    "plt.imshow(captcha)\n",
    "img_w = 69\n",
    "img_h = 24\n",
    "x = np.zeros((1, img_h, img_w, 1))\n",
    "captcha = TextImageGenerator.formatCaptcha(captcha)\n",
    "captcha = np.expand_dims(captcha, axis=0)\n",
    "print(captcha.shape)\n",
    "x[0, :, :captcha.shape[2], :] = captcha\n",
    "ret = decode_batch(test_func, x)\n",
    "print('预测: %s' % ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
